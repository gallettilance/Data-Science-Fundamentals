{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8jfemnP5Jhu",
    "user_expressions": []
   },
   "source": [
    "# Worksheet 17\n",
    "\n",
    "Name: Aru Pandey, Ayush Sharma, Shiven Sharma\n",
    "UID: U14204225, U83105175\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Recommender Systems\n",
    "\n",
    "### Recommender Systems\n",
    "\n",
    "In the example in class of recommending movies to users we used the movie rating as a measure of similarity between users and movies and thus the predicted rating for a user is a proxy for how highly a movie should be recommended. So the higher the predicted rating for a user, the higher a recommendation it would be.\n",
    "\n",
    "a) Consider a streaming platform that only has \"like\" or \"dislike\" (no 1-5 rating). Describe how you would build a recommender system in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rW_yVF9l5Jhv"
   },
   "source": [
    "I can build a recommender system using binary classification with training the user's likes and dislikes data. Then I can find a similar user with the similar classification. We can use these data to predict what they will like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVmZIp2L5Jhv"
   },
   "source": [
    "b) Describe 3 challenges of building a recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJPTLbSJ5Jhv"
   },
   "source": [
    "1. If an user's like history is too short, it is almost impossible to predict what they will like.\n",
    "\n",
    "2. Even if the history is long enough, it can be still difficult if there is no similar user.\n",
    "\n",
    "3. This system can be easily biased. For example, if a user watches a lot of Disney movies, the recommendation system will continue to recommend Disney movies, and the user's overall predisposition will continue to be skewed toward Disney movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1w0KmgVU5Jhv"
   },
   "source": [
    "c) Why is SVD not an option for collaborative filtering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L-lMFS_5Jhv"
   },
   "source": [
    "SVD needs a matrix that is completely filled. However, in collaborative filtering, the ratings matrix is often sparse. SVD may not perform effectively with such sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqQUpT5y5Jhw"
   },
   "source": [
    "d) Use the code below to train a recommender system on a dataset of amazon movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "FPg4DsZO5Jhw",
    "outputId": "db51daca-a4c6-47fa-9114-3a7a6ffbeaa5"
   },
   "outputs": [],
   "source": [
    "# Note: requires py3.10\n",
    "import findspark\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "findspark.init()\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.executor.memory\",\"28g\")\n",
    "conf.set(\"spark.driver.memory\", \"28g\")\n",
    "conf.set(\"spark.driver.cores\", \"8\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "init_df = pd.read_csv(\"./train.csv\").dropna()\n",
    "init_df['UserId_fact'] = init_df['UserId'].astype('category').cat.codes\n",
    "init_df['ProductId_fact'] = init_df['ProductId'].astype('category').cat.codes\n",
    "\n",
    "# Split training set into training and testing set\n",
    "X_train_processed, X_test_processed, Y_train, Y_test = train_test_split(\n",
    "        init_df.drop(['Score'], axis=1),\n",
    "        init_df['Score'],\n",
    "        test_size=1/4.0,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "X_train_processed['Score'] = Y_train\n",
    "df = spark.createDataFrame(X_train_processed[['UserId_fact', 'ProductId_fact', 'Score']])\n",
    "als = ALS(\n",
    "    userCol=\"UserId_fact\",\n",
    "    itemCol=\"ProductId_fact\",\n",
    "    ratingCol=\"Score\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    rank=100\n",
    ")\n",
    "# param_grid = ParamGridBuilder().addGrid(\n",
    "        # als.rank, [10, 50]).addGrid(\n",
    "        # als.regParam, [.1]).addGrid(\n",
    "        # # als.maxIter, [10]).build()\n",
    "# evaluator = RegressionEvaluator(\n",
    "        # metricName=\"rmse\",\n",
    "        # labelCol=\"Score\",\n",
    "        # # predictionCol=\"prediction\")\n",
    "# cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3, parallelism = 6)\n",
    "# cv_fit = cv.fit(df)\n",
    "# rec_sys = cv_fit.bestModel\n",
    "\n",
    "rec_sys = als.fit(df)\n",
    "# rec_sys.save('rec_sys.obj') # so we don't have to re-train it\n",
    "rec = rec_sys.transform(spark.createDataFrame(X_test_processed[['UserId_fact', 'ProductId_fact']])).toPandas()\n",
    "X_test_processed = X_test_processed.merge(rec[['UserId_fact', 'ProductId_fact', 'prediction']],\n",
    "                                          how='left',\n",
    "                                          on=['UserId_fact', 'ProductId_fact'])\n",
    "X_test_processed.rename(columns={'prediction': 'Score'}, inplace=True)\n",
    "X_test_processed['Score'].fillna(X_test_processed['Score'].mean(), inplace=True)\n",
    "X_test_processed['Score'] = X_test_processed['Score'].round().astype(int)\n",
    "\n",
    "print(\"Kaggle RMSE = \", mean_squared_error(X_test_processed['Score'], Y_test, squared=False))\n",
    "\n",
    "cm = confusion_matrix(Y_test, X_test_processed['Score'], normalize='true')\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PrWU_Fn15llH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
